{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyaudi import gdual\n",
    "from pyaudi import sin, cos, tanh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent to train a neural network:\n",
    "\n",
    "    Inputs: 3\n",
    "    Hidden layers: 2 with 5 units/layer\n",
    "    Outputs: 1\n",
    "\n",
    "We need derivatives up to order 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_units = [3, 5, 5, 1]\n",
    "order = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symbolic variables for the weights with values drawn from $\\mathcal N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n_units, order):\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    for layer in range(1, len(n_units)):\n",
    "        weights.append([])\n",
    "        for unit in range(n_units[layer]):\n",
    "            weights[-1].append([])\n",
    "            for prev_unit in range(n_units[layer-1]):                \n",
    "                symname = 'w_{{({0},{1},{2})}}'.format(layer, unit, prev_unit)\n",
    "                w = gdual(np.random.randn(), symname , order)\n",
    "                weights[-1][-1].append(w)\n",
    "          \n",
    "    return weights\n",
    "\n",
    "weights = initialize_weights(n_units, order)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the biases, initialized to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_biases(n_units, order):\n",
    "\n",
    "    biases = []\n",
    "\n",
    "    for layer in range(1, len(n_units)):\n",
    "        biases.append([])\n",
    "        for unit in range(n_units[layer]):\n",
    "            symname = 'b_{{({0},{1})}}'.format(layer, unit)\n",
    "            b = gdual(1, symname , order)\n",
    "            biases[-1].append(b)\n",
    "            \n",
    "    return biases\n",
    "\n",
    "biases = initialize_biases(n_units, order)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function which output is the symbolic expression of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def N_f(inputs, w, b):\n",
    "    \n",
    "    prev_layer_outputs = inputs\n",
    "    \n",
    "    #Hidden layers\n",
    "    for layer in range(len(weights)):\n",
    "        \n",
    "        this_layer_outputs = []\n",
    "        \n",
    "        for unit in range(len(weights[layer])):\n",
    "            \n",
    "            unit_output = 0\n",
    "            unit_output += b[layer][unit]\n",
    "            \n",
    "            for prev_unit,prev_output in enumerate(prev_layer_outputs):\n",
    "                unit_output += w[layer][unit][prev_unit] * prev_output\n",
    "            \n",
    "            if layer != len(weights)-1:\n",
    "                unit_output = tanh(unit_output)\n",
    "                \n",
    "            this_layer_outputs.append(unit_output)\n",
    "            \n",
    "        prev_layer_outputs = this_layer_outputs\n",
    "\n",
    "    return prev_layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define symbolic variables for the inputs and compute the (random) output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N(x) = 0.0583539253975716\n"
     ]
    }
   ],
   "source": [
    "x1 = 1\n",
    "x2 = 2\n",
    "x3 = 4\n",
    "\n",
    "N = N_f([x1,x2, x3], weights, biases)[0]\n",
    "print('N(x) = {0}'.format(N.constant_cf))\n",
    "#N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired output of the network is: $y(\\mathcal x)= x_1x_2 + 0.5x_3 +2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 6.0\n"
     ]
    }
   ],
   "source": [
    "def y_f(x):\n",
    "    return x[0]*x[1] + 0.5*x[2] + 2\n",
    "\n",
    "y = y_f([x1,x2,x3])\n",
    "print('y = {0}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process will seek to minimize a loss function corresponding to the quadratic error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 35.30315807583845\n"
     ]
    }
   ],
   "source": [
    "def loss_f(N,y):\n",
    "    return (N-y)**2\n",
    "\n",
    "loss = loss_f(N, y)\n",
    "print('loss = {0}'.format(loss.constant_cf))\n",
    "#loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the weights with gradient descent using the first order derivatives of the loss function with respect to the weights (and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GD_update(loss, w, b, lr):\n",
    "    \n",
    "    #Update weights\n",
    "    for layer in range(len(w)):\n",
    "        for unit in range(len(w[layer])):\n",
    "            for prev_unit in range(len(w[layer][unit])):\n",
    "                \n",
    "                weight = w[layer][unit][prev_unit]\n",
    "                if weight.symbol_set[0] in loss.symbol_set:\n",
    "                    symbol_idx = loss.symbol_set.index(weight.symbol_set[0])\n",
    "                    d_idx = [0]*loss.symbol_set_size                    \n",
    "                    d_idx[symbol_idx] = 1\n",
    "                    w[layer][unit][prev_unit] -= loss.get_derivative(d_idx) * lr\n",
    "\n",
    "    #Update biases\n",
    "    for i in range(len(b)):\n",
    "        for j in range(len(b[layer])):\n",
    "            \n",
    "                bias = b[layer][unit]\n",
    "                if bias.symbol_set[0] in loss.symbol_set:\n",
    "                    symbol_idx = loss.symbol_set.index(bias.symbol_set[0])\n",
    "                    d_idx = [0]*loss.symbol_set_size                    \n",
    "                    d_idx[symbol_idx] = 1\n",
    "                    b[layer][unit] -= loss.get_derivative(d_idx) * lr\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one GD step the loss function decrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 10.348674990663564\n"
     ]
    }
   ],
   "source": [
    "weights, biases = GD_update(loss, weights, biases, 0.01) \n",
    "\n",
    "N = N_f([x1,x2], weights, biases)[0]\n",
    "loss = loss_f(N, y)\n",
    "print('loss = {0}'.format(loss.constant_cf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same for several data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = -1+2*np.random.rand(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, training loss: 0.11917854414023214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f29615ef978>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEPCAYAAABBUX+lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHxJqggDX5IqwsCXpIow8CWpIv4/gE5W39d2jn0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29615ce748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = []\n",
    "\n",
    "weights = initialize_weights(n_units, order)                \n",
    "biases = initialize_biases(n_units, order)            \n",
    "\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for e in range(epochs):\n",
    "        \n",
    "    #Just records the error at the beginning of this step\n",
    "    epoch_loss = []\n",
    "    for xi in X:\n",
    "        N = N_f(xi, weights, biases)[0]\n",
    "        loss = loss_f(N, y_f(xi))\n",
    "        epoch_loss.append(loss.constant_cf)    \n",
    "        \n",
    "    loss_history.append(np.mean(epoch_loss))\n",
    "    \n",
    "    #Updates the weights\n",
    "    for xi in X:\n",
    "        N = N_f(xi, weights, biases)[0]\n",
    "        loss = loss_f(N, y_f(xi))\n",
    "        weights, biases = GD_update(loss, weights, biases, 0.001)     \n",
    "\n",
    "print('epoch {0}, training loss: {1}'.format(e, loss_history[-1]))        \n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
